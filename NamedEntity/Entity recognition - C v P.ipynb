{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "from bs4 import BeautifulSoup \n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "stop = set(stopwords.words('english'))\n",
    "import string\n",
    "\n",
    "import pickle\n",
    "\n",
    "def dump(obj,filename):\n",
    "    filehandler = open(filename,\"wb\")\n",
    "    pickle.dump(obj,filehandler)\n",
    "    filehandler.close()\n",
    "\n",
    "def load(filename):\n",
    "    file = open(filename,'rb')\n",
    "    obj = pickle.load(file)\n",
    "    file.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOADS CSV WITH SELECTED DATA\n",
    "df = pd.read_csv('conversation_tagged.csv')\n",
    "conversations = df['Sound Bite Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Clean Text used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CLEANS TEXT BY REPLACING CHARACTERS\n",
    "def getCleanText(texts, lower = True):\n",
    "    theList=[]\n",
    "    for text in texts:\n",
    "        text = str(text)\n",
    "        parse_text = BeautifulSoup(text).get_text()\n",
    "        letters_only = re.sub(r'http[s]?:\\/\\/(?:[a-zA-Z]|[0-9]|[$-@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', \n",
    "                              parse_text, flags=re.MULTILINE)\n",
    "        letters_only = re.sub('^(1?[0-9]|2[0-3]):[0-5][0-9]$','time',letters_only)\n",
    "        letters_only = re.sub('(name|NAME)\\d{1,}','username',letters_only)\n",
    "        letters_only = re.sub(\"[^a-zA-Z0-9\\.:']\",  \n",
    "                          \" \",                   \n",
    "                          letters_only)\n",
    "        letters_only = letters_only.replace('\\n',\" \")\n",
    "        letters_only = letters_only.replace('\\r',\" \")\n",
    "        letters_only = re.sub('[.]{2,}', '. ', letters_only)\n",
    "        if lower:\n",
    "            letters_only = letters_only.lower()\n",
    "        words = CountVectorizer(stop_words='english').build_tokenizer()(letters_only)\n",
    "        meaningful_words = [ w for w in words if len(w)> 0 and len(w)<20]\n",
    "        clean_text = \" \".join(meaningful_words)\n",
    "        theList.append(clean_text)\n",
    "    finalList = \"_\".join(theList)\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define some parameters  \n",
    "noisy_pos_tags = ['PROP'] # TAGGING OF PROP\n",
    "min_token_length = 2 # TWO CHARACTERS OR TWO TOKENS?\n",
    "\n",
    "#Function to check if the token is a noise or not  \n",
    "def isNoise(token):     \n",
    "    is_noise = False\n",
    "    if token.pos_ in noisy_pos_tags: # FROM LIST ABOVE\n",
    "        is_noise = True \n",
    "    elif token.is_stop == True: # FROM STOP WORDS LIST \n",
    "        is_noise = True\n",
    "    elif len(token.string) <= min_token_length: # FROM PARAMETER ABOVE\n",
    "        is_noise = True\n",
    "    return is_noise \n",
    "def cleanup(token, lower = True):\n",
    "    if lower: # TURNS ALL TEXT TO LOWERCASE\n",
    "        token = token.lower()\n",
    "    return token.strip() # REMOVES SPACES FROM THE BEGINNING AND END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    try:\n",
    "        document = nlp(sent)\n",
    "    except:\n",
    "        return [\"error\"]\n",
    "    \n",
    "    entities_list = []\n",
    "    labels = set([w.label_ for w in document.ents]) \n",
    "    for label in labels: \n",
    "        entities = [cleanup(e.string, lower=False) for e in document.ents if label==e.label_] \n",
    "        entities = list(set(entities))\n",
    "        if label == \"ORG\": # PICKS ORG ENTITIES\n",
    "            entities_list.extend(entities)\n",
    "    if len(entities_list) > 0:\n",
    "        return entities_list\n",
    "    else:\n",
    "        return [\"<UNK>\"]\n",
    "org_list = map(get_entities, conversations)\n",
    "org_list = list(org_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatentate Named Entities together from all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/k/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "all_orgs_clean = map(getCleanText, org_list)\n",
    "entities_per_text = list(all_orgs_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of Named Entitites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_dict = Counter(entities_per_text)\n",
    "entities_list_sorted = sorted(entities_dict, key = entities_dict.get, reverse=True)\n",
    "entities_list_sorted_tup = [(entity,entities_dict[entity]) for entity in entities_list_sorted]\n",
    "#entities_list_sorted_tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add list as column and Check for consitency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18340, 17)\n",
      "18340\n"
     ]
    }
   ],
   "source": [
    "df = df.assign(entities = entities_per_text)\n",
    "print(df.shape)\n",
    "print(len(entities_per_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pepsico inc_elise amendola_pepsi_3q_file ap photo_pepsico inc_the associated press_mobile apps file_file ap', 'pepsico inc_elise amendola_pepsi_3q_file ap photo_pepsico inc_the associated press_mobile apps file_file ap', 'the district of columbia_chrysler_mcdonald_coca cola_executive creative director ashley sword_stellar novellas_stellar scholars_roundtable_stellar romance', 'crisps_coca cola_walkers', 'coca cola_instagram']\n"
     ]
    }
   ],
   "source": [
    "print(entities_per_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dump files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump(df, \"../dumps/df_sel_entities.pkl\")\n",
    "dump(entities_list_sorted_tup, \"../dumps/df__sel_entities_count.pkl\")\n",
    "dump(org_list, \"../dumps/df_sel_entities_list.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
